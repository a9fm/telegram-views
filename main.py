#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import aiohttp
import asyncio
from aiohttp_socks import ProxyConnector
import re
import random
import os
from datetime import datetime
from fake_useragent import UserAgent

# ============================================
# üìã –ù–ê–°–¢–†–û–ô–ö–ò
# ============================================
WORKING_FILE = "working.txt"
DEAD_FILE = "dead.txt"
POSTS_COUNT = 3
CONCURRENCY = 100
PROXY_TIMEOUT = 5
MAX_USES_PER_PROXY = 5  # –ö–∞–∂–¥—ã–π –ø—Ä–æ–∫—Å–∏ –º–æ–∂–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å 5 —Ä–∞–∑

# –ü—É—Ç–∏ –∫ —Ñ–∞–π–ª–∞–º —Å –∏—Å—Ç–æ—á–Ω–∏–∫–∞–º–∏
AUTO_HTTP = "auto/http.txt"
AUTO_SOCKS4 = "auto/socks4.txt"
AUTO_SOCKS5 = "auto/socks5.txt"

# ============================================
# üìä –°–¢–ê–¢–ò–°–¢–ò–ö–ê
# ============================================
stats = {
    'tested': 0,
    'working': 0,
    'dead': 0,
    'views_sent': 0,
    'parsed': 0,
    'start_time': datetime.now()
}

# ============================================
# üîß –í–°–ü–û–ú–û–ì–ê–¢–ï–õ–¨–ù–´–ï
# ============================================
def log(message):
    print(f"[{datetime.now().strftime('%H:%M:%S')}] {message}")

def update_progress():
    elapsed = (datetime.now() - stats['start_time']).total_seconds()
    speed = stats['tested'] / elapsed if elapsed > 0 else 0
    print(f"\rüìä –ü—Ä–æ–≥—Ä–µ—Å—Å: ‚úÖ {stats['working']} | üíÄ {stats['dead']} | üëÅÔ∏è {stats['views_sent']} | üì• {stats['parsed']} | ‚ö° {speed:.1f}/—Å | –í—Ä–µ–º—è: {elapsed:.0f}—Å", end="", flush=True)

def load_working_proxies():
    if os.path.exists(WORKING_FILE):
        with open(WORKING_FILE, "r") as f:
            return [line.strip() for line in f if line.strip()]
    return []

def save_working_proxy(proxy):
    with open(WORKING_FILE, "a") as f:
        f.write(proxy + "\n")
    stats['working'] += 1

def save_dead_proxy(proxy):
    with open(DEAD_FILE, "a") as f:
        f.write(proxy + "\n")
    stats['dead'] += 1

def load_dead_proxies():
    if os.path.exists(DEAD_FILE):
        with open(DEAD_FILE, "r") as f:
            return set(line.strip() for line in f if line.strip())
    return set()

# ============================================
# üìñ –ß–¢–ï–ù–ò–ï –ò–°–¢–û–ß–ù–ò–ö–û–í –ò–ó –§–ê–ô–õ–û–í
# ============================================
def load_source_urls():
    sources = {'http': [], 'socks4': [], 'socks5': []}
    
    if os.path.exists(AUTO_HTTP):
        with open(AUTO_HTTP, 'r') as f:
            sources['http'] = [line.strip() for line in f if line.strip() and not line.startswith('#')]
        log(f"üìÅ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(sources['http'])} HTTP –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤")
    
    if os.path.exists(AUTO_SOCKS4):
        with open(AUTO_SOCKS4, 'r') as f:
            sources['socks4'] = [line.strip() for line in f if line.strip() and not line.startswith('#')]
        log(f"üìÅ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(sources['socks4'])} SOCKS4 –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤")
    
    if os.path.exists(AUTO_SOCKS5):
        with open(AUTO_SOCKS5, 'r') as f:
            sources['socks5'] = [line.strip() for line in f if line.strip() and not line.startswith('#')]
        log(f"üìÅ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(sources['socks5'])} SOCKS5 –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤")
    
    return sources

# ============================================
# üåê –ü–ê–†–°–ò–ù–ì –ü–†–û–ö–°–ò –° –ò–°–¢–û–ß–ù–ò–ö–û–í
# ============================================
async def parse_proxies_from_url(source_url: str, proxy_type: str):
    try:
        async with aiohttp.ClientSession() as session:
            async with session.get(source_url, timeout=15) as resp:
                if resp.status == 200:
                    text = await resp.text()
                    pattern = r'\b(?:\d{1,3}\.){3}\d{1,3}:\d{2,5}\b'
                    proxies = re.findall(pattern, text)
                    result = [f"{proxy_type}://{p}" for p in proxies]
                    log(f"üì• {source_url.split('/')[-1]}: {len(result)} {proxy_type} –ø—Ä–æ–∫—Å–∏")
                    return result
    except:
        pass
    return []

async def parse_all_proxies():
    log("üîç –ù–∞—á–∏–Ω–∞—é –ø–∞—Ä—Å–∏–Ω–≥ –ø—Ä–æ–∫—Å–∏ –∏–∑ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤...")
    
    sources = load_source_urls()
    all_proxies = []
    tasks = []
    
    for proxy_type, urls in sources.items():
        for url in urls:
            tasks.append(parse_proxies_from_url(url, proxy_type))
    
    results = await asyncio.gather(*tasks)
    
    for proxies in results:
        all_proxies.extend(proxies)
    
    unique = list(set(all_proxies))
    stats['parsed'] = len(unique)
    
    dead_set = load_dead_proxies()
    alive = [p for p in unique if p not in dead_set]
    
    log(f"üìä –í—Å–µ–≥–æ: {len(all_proxies)} ‚Üí —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö: {len(unique)} ‚Üí –±–µ–∑ –º–µ—Ä—Ç–≤—ã—Ö: {len(alive)}")
    return alive

# ============================================
# üîç –ü–†–û–í–ï–†–ö–ê –ü–†–û–ö–°–ò
# ============================================
async def check_proxy(proxy_url: str, test_url: str):
    try:
        connector = ProxyConnector.from_url(proxy_url, rdns=True)
        async with aiohttp.ClientSession(connector=connector) as session:
            headers = {"User-Agent": UserAgent().random}
            async with session.get(test_url, headers=headers, timeout=aiohttp.ClientTimeout(total=PROXY_TIMEOUT)) as response:
                if response.status == 200:
                    html = await response.text()
                    if 'data-view="' in html:
                        return True
    except:
        pass
    return False

async def test_proxies_batch(proxies, test_url):
    log(f"üß™ –¢–µ—Å—Ç–∏—Ä—É—é {len(proxies)} –ø—Ä–æ–∫—Å–∏...")
    
    dead_set = load_dead_proxies()
    proxies = [p for p in proxies if p not in dead_set]
    log(f"üìâ –ü–æ—Å–ª–µ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏ –º–µ—Ä—Ç–≤—ã—Ö: {len(proxies)}")
    
    semaphore = asyncio.Semaphore(200)
    working = []
    
    async def test_one(proxy):
        async with semaphore:
            stats['tested'] += 1
            if await check_proxy(proxy, test_url):
                save_working_proxy(proxy)
                working.append(proxy)
                update_progress()
                return True
            else:
                save_dead_proxy(proxy)
                update_progress()
                return False
    
    chunk_size = 1000
    for i in range(0, len(proxies), chunk_size):
        chunk = proxies[i:i+chunk_size]
        tasks = [test_one(p) for p in chunk]
        await asyncio.gather(*tasks)
        log(f"üìä –ß–∞–Ω–∫ {i//chunk_size + 1}: –Ω–∞–π–¥–µ–Ω–æ {len(working)} —Ä–∞–±–æ—á–∏—Ö")
        await asyncio.sleep(1)
    
    log(f"\n‚úÖ –ù–∞–π–¥–µ–Ω–æ {len(working)} —Ä–∞–±–æ—á–∏—Ö –ø—Ä–æ–∫—Å–∏")
    return working

# ============================================
# üéØ –û–¢–ü–†–ê–í–ö–ê –ü–†–û–°–ú–û–¢–†–ê
# ============================================
async def send_view(channel: str, post_id: int, proxy_url: str = None):
    try:
        connector = None
        if proxy_url:
            connector = ProxyConnector.from_url(proxy_url, rdns=True)
        
        async with aiohttp.ClientSession(connector=connector) as session:
            ua = UserAgent().random
            
            headers = {
                "User-Agent": ua,
                "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",
                "Accept-Language": "ru-RU,ru;q=0.9,en;q=0.8",
            }
            
            embed_url = f"https://t.me/{channel}/{post_id}?embed=1&mode=tme"
            
            async with session.get(embed_url, headers=headers, timeout=PROXY_TIMEOUT) as resp:
                if resp.status != 200:
                    return False
                
                html = await resp.text()
                token_match = re.search(r'data-view="([^"]+)"', html)
                if not token_match:
                    return False
                
                token = token_match.group(1)
                
                view_headers = {
                    "User-Agent": ua,
                    "Referer": embed_url,
                    "X-Requested-With": "XMLHttpRequest",
                }
                
                async with session.post(
                    f"https://t.me/v/?views={token}",
                    headers=view_headers,
                    timeout=PROXY_TIMEOUT
                ) as view_resp:
                    
                    if view_resp.status == 200:
                        text = await view_resp.text()
                        if text == "true":
                            stats['views_sent'] += 1
                            update_progress()
                            return True
        return False
    except:
        return False

# ============================================
# üåê –ü–ê–†–°–ò–ù–ì –ü–û–°–¢–û–í
# ============================================
async def get_last_posts(channel: str):
    url = f"https://t.me/s/{channel}"
    
    async with aiohttp.ClientSession() as session:
        try:
            headers = {"User-Agent": UserAgent().random}
            async with session.get(url, headers=headers, timeout=10) as resp:
                html = await resp.text()
                
                pattern = rf'data-post="{channel}/(\d+)"'
                post_ids = re.findall(pattern, html)
                
                if not post_ids:
                    pattern = rf'href="/{channel}/(\d+)"'
                    post_ids = re.findall(pattern, html)
                
                if post_ids:
                    unique = sorted(set(int(id) for id in post_ids))
                    last = unique[-POSTS_COUNT:]
                    log(f"üì° –ù–∞–π–¥–µ–Ω–æ –ø–æ—Å—Ç–æ–≤: {last}")
                    return last
        except Exception as e:
            log(f"‚ùå –û—à–∏–±–∫–∞: {e}")
    return []

# ============================================
# üöÄ –†–ï–ñ–ò–ú LIST (–ë–ï–°–ö–û–ù–ï–ß–ù–ê–Ø –ù–ê–ö–†–£–¢–ö–ê)
# ============================================
async def list_mode(channel: str):
    """–†–µ–∂–∏–º LIST - –±–µ—Å–∫–æ–Ω–µ—á–Ω–∞—è –Ω–∞–∫—Ä—É—Ç–∫–∞, 5 –ø–æ–ø—ã—Ç–æ–∫ –Ω–∞ –ø—Ä–æ–∫—Å–∏"""
    log("üöÄ –ó–∞–ø—É—â–µ–Ω LIST —Ä–µ–∂–∏–º (–±–µ—Å–∫–æ–Ω–µ—á–Ω–∞—è –Ω–∞–∫—Ä—É—Ç–∫–∞)")
    
    post_ids = await get_last_posts(channel)
    if not post_ids:
        log("‚ùå –ù–µ—Ç –ø–æ—Å—Ç–æ–≤")
        return
    
    proxies = load_working_proxies()
    if not proxies:
        log("‚ùå –ù–µ—Ç –ø—Ä–æ–∫—Å–∏ –≤ working.txt")
        return
    
    log(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(proxies)} –ø—Ä–æ–∫—Å–∏")
    log(f"üéØ –ü–æ—Å—Ç—ã: {post_ids}")
    log(f"üîÑ –ö–∞–∂–¥—ã–π –ø—Ä–æ–∫—Å–∏ –±—É–¥–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω –º–∞–∫—Å–∏–º—É–º {MAX_USES_PER_PROXY} —Ä–∞–∑")
    
    # –°–ª–æ–≤–∞—Ä—å –¥–ª—è –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–Ω—ã—Ö –ø—Ä–æ–∫—Å–∏
    proxy_usage = {proxy: 0 for proxy in proxies}
    total_attempts = 0
    successful_views = 0
    
    try:
        while True:
            # –í—ã–±–∏—Ä–∞–µ–º –ø—Ä–æ–∫—Å–∏ –∫–æ—Ç–æ—Ä—ã–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–ª–∏—Å—å –º–µ–Ω—å—à–µ MAX_USES_PER_PROXY —Ä–∞–∑
            available_proxies = [p for p in proxies if proxy_usage[p] < MAX_USES_PER_PROXY]
            
            if not available_proxies:
                log(f"‚ùå –í—Å–µ –ø—Ä–æ–∫—Å–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω—ã {MAX_USES_PER_PROXY} —Ä–∞–∑. –ó–∞–≤–µ—Ä—à–∞—é —Ä–∞–±–æ—Ç—É.")
                break
            
            # –°–æ–∑–¥–∞–µ–º –∑–∞–¥–∞—á–∏ –¥–ª—è –≤—Å–µ—Ö –ø–æ—Å—Ç–æ–≤
            tasks = []
            for post_id in post_ids:
                proxy = random.choice(available_proxies)
                proxy_usage[proxy] += 1
                total_attempts += 1
                tasks.append(send_view(channel, post_id, proxy))
            
            # –ó–∞–ø—É—Å–∫–∞–µ–º –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ
            results = await asyncio.gather(*tasks, return_exceptions=True)
            
            # –°—á–∏—Ç–∞–µ–º —É—Å–ø–µ—Ö–∏
            success = sum(1 for r in results if r is True)
            successful_views += success
            
            # –õ–æ–≥–∏—Ä—É–µ–º –ø—Ä–æ–≥—Ä–µ—Å—Å
            success_rate = (successful_views / total_attempts * 100) if total_attempts > 0 else 0
            log(f"üëÅÔ∏è –£—Å–ø–µ—à–Ω–æ: {successful_views} | –ü–æ–ø—ã—Ç–æ–∫: {total_attempts} | {success_rate:.1f}% | –û—Å—Ç–∞–ª–æ—Å—å –ø—Ä–æ–∫—Å–∏: {len(available_proxies)}")
            
            # –ù–µ–±–æ–ª—å—à–∞—è –ø–∞—É–∑–∞ –º–µ–∂–¥—É —Ü–∏–∫–ª–∞–º–∏
            await asyncio.sleep(0.5)
            
    except KeyboardInterrupt:
        log("\nüõë –û—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º")
    
    # –ò—Ç–æ–≥
    elapsed = (datetime.now() - stats['start_time']).total_seconds()
    success_rate = (successful_views / total_attempts * 100) if total_attempts > 0 else 0
    
    log(f"\nüìä –ò–¢–û–ì:")
    log(f"‚úÖ –í—Å–µ–≥–æ –ø—Ä–æ—Å–º–æ—Ç—Ä–æ–≤: {successful_views}")
    log(f"üîÑ –í—Å–µ–≥–æ –ø–æ–ø—ã—Ç–æ–∫: {total_attempts}")
    log(f"üìà –ü—Ä–æ—Ü–µ–Ω—Ç —É—Å–ø–µ—Ö–∞: {success_rate:.1f}%")
    log(f"‚è±Ô∏è –í—Ä–µ–º—è —Ä–∞–±–æ—Ç—ã: {elapsed:.1f}—Å")

# ============================================
# üöÄ –†–ï–ñ–ò–ú AUTO
# ============================================
async def auto_mode(channel: str):
    log("üöÄ –ó–∞–ø—É—â–µ–Ω AUTO —Ä–µ–∂–∏–º")
    
    post_ids = await get_last_posts(channel)
    if not post_ids:
        log("‚ùå –ù–µ—Ç –ø–æ—Å—Ç–æ–≤")
        return
    
    fresh_proxies = await parse_all_proxies()
    if not fresh_proxies:
        log("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å —Å–ø–∞—Ä—Å–∏—Ç—å –ø—Ä–æ–∫—Å–∏")
        return
    
    test_url = f"https://t.me/{channel}/{post_ids[0]}?embed=1&mode=tme"
    working = await test_proxies_batch(fresh_proxies, test_url)
    
    if not working:
        log("‚ùå –ù–µ—Ç —Ä–∞–±–æ—á–∏—Ö –ø—Ä–æ–∫—Å–∏")
        return
    
    # –ü–æ—Å–ª–µ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –∑–∞–ø—É—Å–∫–∞–µ–º list_mode —Å –Ω–æ–≤—ã–º–∏ –ø—Ä–æ–∫—Å–∏
    global proxies
    proxies = working
    await list_mode(channel)

# ============================================
# üìå –¢–û–ß–ö–ê –í–•–û–î–ê
# ============================================
if __name__ == "__main__":
    import sys
    
    channel = input("üì¢ –ö–∞–Ω–∞–ª (–±–µ–∑ @): ").strip()
    
    print("\n1. Auto —Ä–µ–∂–∏–º (–ø–∞—Ä—Å–∏–Ω–≥ –∏–∑ auto/ + —Ç–µ—Å—Ç + –Ω–∞–∫—Ä—É—Ç–∫–∞)")
    print("2. List —Ä–µ–∂–∏–º (—Ç–æ–ª—å–∫–æ –Ω–∞–∫—Ä—É—Ç–∫–∞ –∏–∑ working.txt)")
    choice = input("\n–í—ã–±–µ—Ä–∏ (1/2): ").strip()
    
    print("=" * 50)
    mode = "AUTO" if choice == "1" else "LIST"
    print(f"ü§ñ Telegram Views Bot - {mode} —Ä–µ–∂–∏–º")
    print("=" * 50)
    
    stats['start_time'] = datetime.now()
    
    if choice == "1":
        asyncio.run(auto_mode(channel))
    else:
        asyncio.run(list_mode(channel))
