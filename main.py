#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import aiohttp
import asyncio
from aiohttp_socks import ProxyConnector
import re
import random
import os
from datetime import datetime
from fake_useragent import UserAgent
import json

# ============================================
# üìã –ù–ê–°–¢–†–û–ô–ö–ò
# ============================================
WORKING_FILE = "working.txt"
DEAD_FILE = "dead.txt"
POSTS_COUNT = 3
VIEWS_PER_POST = 10
CONCURRENCY = 100
PROXY_TIMEOUT = 5

# ============================================
# üìä –°–¢–ê–¢–ò–°–¢–ò–ö–ê
# ============================================
stats = {
    'tested': 0,
    'working': 0,
    'dead': 0,
    'views_sent': 0,
    'start_time': datetime.now()
}

# ============================================
# üîß –í–°–ü–û–ú–û–ì–ê–¢–ï–õ–¨–ù–´–ï –§–£–ù–ö–¶–ò–ò
# ============================================
def log(message):
    timestamp = datetime.now().strftime("%H:%M:%S")
    print(f"[{timestamp}] {message}")

def update_progress():
    elapsed = (datetime.now() - stats['start_time']).total_seconds()
    speed = stats['tested'] / elapsed if elapsed > 0 else 0
    print(f"\rüìä –ü—Ä–æ–≥—Ä–µ—Å—Å: ‚úÖ {stats['working']} | üíÄ {stats['dead']} | üëÅÔ∏è {stats['views_sent']} | ‚ö° {speed:.1f}/—Å | –í—Ä–µ–º—è: {elapsed:.0f}—Å", end="", flush=True)

def load_working_proxies():
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç —Ä–∞–±–æ—á–∏–µ –ø—Ä–æ–∫—Å–∏ –∏–∑ —Ñ–∞–π–ª–∞"""
    if not os.path.exists(WORKING_FILE):
        return []
    try:
        with open(WORKING_FILE, "r", encoding='utf-8', errors='ignore') as f:
            proxies = [line.strip() for line in f if line.strip()]
        log(f"üìÅ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(proxies)} —Ä–∞–±–æ—á–∏—Ö –ø—Ä–æ–∫—Å–∏ –∏–∑ {WORKING_FILE}")
        return proxies
    except Exception as e:
        log(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏: {e}")
        return []

def save_working_proxy(proxy_str):
    """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç —Ä–∞–±–æ—á–∏–π –ø—Ä–æ–∫—Å–∏"""
    try:
        with open(WORKING_FILE, "a", encoding='utf-8', errors='ignore') as f:
            f.write(proxy_str + "\n")
        stats['working'] += 1
        log(f"üíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω —Ä–∞–±–æ—á–∏–π: {proxy_str}")
        update_progress()
    except Exception as e:
        log(f"‚ùå –û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è: {e}")

def save_dead_proxy(proxy_str):
    """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç –º–µ—Ä—Ç–≤—ã–π –ø—Ä–æ–∫—Å–∏"""
    try:
        with open(DEAD_FILE, "a", encoding='utf-8', errors='ignore') as f:
            f.write(proxy_str + "\n")
        stats['dead'] += 1
        update_progress()
    except Exception:
        pass

# ============================================
# üîç –ü–†–û–í–ï–†–ö–ê –ü–†–û–ö–°–ò
# ============================================
async def check_proxy(proxy_url: str, test_url: str):
    """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç —Ä–∞–±–æ—Ç–∞–µ—Ç –ª–∏ –ø—Ä–æ–∫—Å–∏ —Å Telegram"""
    try:
        connector = ProxyConnector.from_url(proxy_url, rdns=True)
        async with aiohttp.ClientSession(connector=connector) as session:
            headers = {
                "User-Agent": UserAgent().random,
                "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",
                "Accept-Language": "ru-RU,ru;q=0.9,en;q=0.8",
                "Connection": "keep-alive",
            }
            
            async with session.get(
                test_url,
                headers=headers,
                timeout=aiohttp.ClientTimeout(total=PROXY_TIMEOUT)
            ) as response:
                
                if response.status == 200:
                    html = await response.text()
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ —Ç–æ–∫–µ–Ω–∞ –ø—Ä–æ—Å–º–æ—Ç—Ä–∞
                    if 'data-view="' in html:
                        return True
        return False
    except Exception as e:
        return False

# ============================================
# üéØ –û–¢–ü–†–ê–í–ö–ê –ü–†–û–°–ú–û–¢–†–ê
# ============================================
async def send_view(channel: str, post_id: int, proxy_url: str = None):
    """–û—Ç–ø—Ä–∞–≤–ª—è–µ—Ç –ø—Ä–æ—Å–º–æ—Ç—Ä —á–µ—Ä–µ–∑ –ø—Ä–æ–∫—Å–∏"""
    try:
        connector = None
        if proxy_url:
            connector = ProxyConnector.from_url(proxy_url, rdns=True)
        
        async with aiohttp.ClientSession(connector=connector) as session:
            # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º User-Agent
            ua = UserAgent().random
            
            # 1. –ü–æ–ª—É—á–∞–µ–º —Å—Ç—Ä–∞–Ω–∏—Ü—É —Å —Ç–æ–∫–µ–Ω–æ–º
            headers = {
                "User-Agent": ua,
                "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",
                "Accept-Language": "ru-RU,ru;q=0.9,en;q=0.8",
                "Accept-Encoding": "gzip, deflate, br",
                "Connection": "keep-alive",
                "Upgrade-Insecure-Requests": "1",
                "Sec-Fetch-Dest": "document",
                "Sec-Fetch-Mode": "navigate",
                "Sec-Fetch-Site": "none",
                "Sec-Fetch-User": "?1",
                "Cache-Control": "max-age=0",
            }
            
            embed_url = f"https://t.me/{channel}/{post_id}?embed=1&mode=tme"
            
            async with session.get(
                embed_url,
                headers=headers,
                timeout=aiohttp.ClientTimeout(total=PROXY_TIMEOUT)
            ) as resp:
                
                if resp.status != 200:
                    return False
                
                html = await resp.text()
                
                # –ò—â–µ–º —Ç–æ–∫–µ–Ω –ø—Ä–æ—Å–º–æ—Ç—Ä–∞
                token_match = re.search(r'data-view="([^"]+)"', html)
                if not token_match:
                    return False
                
                token = token_match.group(1)
                
                # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∫—É–∫–∏
                cookies = resp.cookies
                
                # 2. –û—Ç–ø—Ä–∞–≤–ª—è–µ–º –ø—Ä–æ—Å–º–æ—Ç—Ä
                view_headers = {
                    "User-Agent": ua,
                    "Accept": "*/*",
                    "Accept-Language": "ru-RU,ru;q=0.9,en;q=0.8",
                    "Accept-Encoding": "gzip, deflate, br",
                    "Referer": embed_url,
                    "X-Requested-With": "XMLHttpRequest",
                    "Connection": "keep-alive",
                    "Sec-Fetch-Dest": "empty",
                    "Sec-Fetch-Mode": "cors",
                    "Sec-Fetch-Site": "same-origin",
                }
                
                view_url = f"https://t.me/v/?views={token}"
                
                async with session.post(
                    view_url,
                    headers=view_headers,
                    cookies=cookies,
                    timeout=aiohttp.ClientTimeout(total=PROXY_TIMEOUT)
                ) as view_resp:
                    
                    if view_resp.status == 200:
                        text = await view_resp.text()
                        if text == "true":
                            stats['views_sent'] += 1
                            update_progress()
                            return True
                        else:
                            log(f"‚ö†Ô∏è –û—Ç–≤–µ—Ç: {text}")
        
        return False
        
    except Exception as e:
        return False

# ============================================
# üåê –ü–ê–†–°–ò–ù–ì –ü–û–°–¢–û–í –ö–ê–ù–ê–õ–ê
# ============================================
async def get_last_posts(channel: str, count: int = POSTS_COUNT):
    """–ü–æ–ª—É—á–∞–µ—Ç –ø–æ—Å–ª–µ–¥–Ω–∏–µ –ø–æ—Å—Ç—ã –∫–∞–Ω–∞–ª–∞"""
    url = f"https://t.me/s/{channel}"
    
    async with aiohttp.ClientSession() as session:
        try:
            headers = {"User-Agent": UserAgent().random}
            
            async with session.get(url, headers=headers, timeout=10) as response:
                html = await response.text()
                
                # –ò—â–µ–º ID –ø–æ—Å—Ç–æ–≤ –≤ —Ä–∞–∑–Ω—ã—Ö —Ñ–æ—Ä–º–∞—Ç–∞—Ö
                patterns = [
                    rf'data-post="{channel}/(\d+)"',
                    rf'href="/{channel}/(\d+)"',
                    rf'data-post="//t.me/{channel}/(\d+)"',
                ]
                
                post_ids = []
                for pattern in patterns:
                    found = re.findall(pattern, html)
                    post_ids.extend(found)
                
                if not post_ids:
                    log("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –Ω–∞–π—Ç–∏ –ø–æ—Å—Ç—ã")
                    return []
                
                # –£–±–∏—Ä–∞–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã –∏ —Å–æ—Ä—Ç–∏—Ä—É–µ–º
                unique_ids = sorted(set(int(id) for id in post_ids))
                
                # –ë–µ—Ä–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–µ count
                last_posts = unique_ids[-count:]
                
                log(f"üì° –ù–∞–π–¥–µ–Ω–æ –ø–æ—Å—Ç–æ–≤: {last_posts}")
                return last_posts
                
        except Exception as e:
            log(f"‚ùå –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ –∫–∞–Ω–∞–ª–∞: {e}")
            return []

# ============================================
# üöÄ –ó–ê–ü–£–°–ö –ù–ê–ö–†–£–¢–ö–ò
# ============================================
async def run_views(channel: str, post_ids: list, proxies: list):
    """–ó–∞–ø—É—Å–∫–∞–µ—Ç –Ω–∞–∫—Ä—É—Ç–∫—É –Ω–∞ –≤—Å–µ –ø–æ—Å—Ç—ã"""
    
    # –°–æ–∑–¥–∞–µ–º –∑–∞–¥–∞—á–∏ –¥–ª—è –≤—Å–µ—Ö –ø–æ—Å—Ç–æ–≤
    all_tasks = []
    for post_id in post_ids:
        for _ in range(VIEWS_PER_POST):
            proxy = random.choice(proxies) if proxies else None
            all_tasks.append(send_view(channel, post_id, proxy))
    
    log(f"üöÄ –ó–∞–ø—É—Å–∫ {len(all_tasks)} –ø—Ä–æ—Å–º–æ—Ç—Ä–æ–≤...")
    
    # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ—Å—Ç—å
    semaphore = asyncio.Semaphore(CONCURRENCY)
    
    async def run_with_limit(task):
        async with semaphore:
            return await task
    
    limited_tasks = [run_with_limit(task) for task in all_tasks]
    
    # –ó–∞–ø—É—Å–∫–∞–µ–º
    results = await asyncio.gather(*limited_tasks, return_exceptions=True)
    
    # –°—á–∏—Ç–∞–µ–º —É—Å–ø–µ—Ö–∏
    success = sum(1 for r in results if r is True)
    
    return success, len(all_tasks)

# ============================================
# üöÄ –ì–õ–ê–í–ù–ê–Ø –§–£–ù–ö–¶–ò–Ø
# ============================================
async def main():
    print("=" * 50)
    print("ü§ñ TELEGRAM VIEWS BOT - –°–¢–ê–†–ê–Ø –†–ê–ë–û–ß–ê–Ø –í–ï–†–°–ò–Ø")
    print("=" * 50)
    
    # –í–≤–æ–¥ –∫–∞–Ω–∞–ª–∞
    channel = input("üì¢ –í–≤–µ–¥–∏—Ç–µ –∫–∞–Ω–∞–ª (–±–µ–∑ @): ").strip()
    if not channel:
        channel = "a9fm_price"
        log(f"‚ö†Ô∏è –ò—Å–ø–æ–ª—å–∑—É—é {channel}")
    
    # –ü–æ–ª—É—á–∞–µ–º –ø–æ—Å—Ç—ã
    post_ids = await get_last_posts(channel, POSTS_COUNT)
    if not post_ids:
        log("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –ø–æ—Å—Ç—ã")
        return
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º –ø—Ä–æ–∫—Å–∏
    proxies = load_working_proxies()
    if not proxies:
        log("‚ùå –ù–µ—Ç –ø—Ä–æ–∫—Å–∏ –≤ working.txt")
        return
    
    log(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(proxies)} –ø—Ä–æ–∫—Å–∏")
    log(f"üéØ –ü–æ—Å—Ç—ã: {post_ids}")
    
    # –ó–∞–ø—É—Å–∫–∞–µ–º –Ω–∞–∫—Ä—É—Ç–∫—É
    success, total = await run_views(channel, post_ids, proxies)
    
    # –ò—Ç–æ–≥
    elapsed = (datetime.now() - stats['start_time']).total_seconds()
    print("\n" + "=" * 50)
    print("üèÅ –†–ï–ó–£–õ–¨–¢–ê–¢–´")
    print(f"‚úÖ –£—Å–ø–µ—à–Ω–æ: {success}/{total}")
    print(f"üëÅÔ∏è –ü—Ä–æ—Å–º–æ—Ç—Ä–æ–≤ –∑–∞—Å—á–∏—Ç–∞–Ω–æ: {stats['views_sent']}")
    print(f"üìä –†–∞–±–æ—á–∏—Ö –ø—Ä–æ–∫—Å–∏: {stats['working']}")
    print(f"üíÄ –ú–µ—Ä—Ç–≤—ã—Ö –ø—Ä–æ–∫—Å–∏: {stats['dead']}")
    print(f"‚è±Ô∏è –í—Ä–µ–º—è: {elapsed:.1f}—Å")
    print("=" * 50)

# ============================================
# üî• –¢–ï–°–¢ –ü–†–û–ö–°–ò (–û–¢–î–ï–õ–¨–ù–û)
# ============================================
async def test_proxies_mode():
    """–†–µ–∂–∏–º —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –ø—Ä–æ–∫—Å–∏"""
    print("=" * 50)
    print("üîç –¢–ï–°–¢–ò–†–û–í–ê–ù–ò–ï –ü–†–û–ö–°–ò")
    print("=" * 50)
    
    channel = input("üì¢ –ö–∞–Ω–∞–ª –¥–ª—è —Ç–µ—Å—Ç–∞: ").strip() or "a9fm_price"
    post_id = input("üì¢ –ü–æ—Å—Ç –¥–ª—è —Ç–µ—Å—Ç–∞: ").strip() or "816"
    
    test_url = f"https://t.me/{channel}/{post_id}?embed=1&mode=tme"
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º –ø—Ä–æ–∫—Å–∏ –∏–∑ —Ñ–∞–π–ª–∞
    if not os.path.exists("proxies.txt"):
        log("‚ùå –ù–µ—Ç —Ñ–∞–π–ª–∞ proxies.txt")
        return
    
    with open("proxies.txt", "r") as f:
        proxies = [line.strip() for line in f if line.strip()]
    
    log(f"üìÅ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(proxies)} –ø—Ä–æ–∫—Å–∏ –¥–ª—è —Ç–µ—Å—Ç–∞")
    
    # –¢–µ—Å—Ç–∏—Ä—É–µ–º
    semaphore = asyncio.Semaphore(50)
    
    async def test_one(proxy):
        async with semaphore:
            stats['tested'] += 1
            if await check_proxy(proxy, test_url):
                save_working_proxy(proxy)
                return True
            else:
                save_dead_proxy(proxy)
                return False
    
    tasks = [test_one(p) for p in proxies[:500]]  # –¢–µ—Å—Ç–∏—Ä—É–µ–º –ø–µ—Ä–≤—ã–µ 500
    results = await asyncio.gather(*tasks)
    
    log(f"\n‚úÖ –¢–µ—Å—Ç –∑–∞–≤–µ—Ä—à–µ–Ω: {stats['working']} —Ä–∞–±–æ—á–∏—Ö –∏–∑ {len(proxies[:500])}")

# ============================================
# üìå –¢–û–ß–ö–ê –í–•–û–î–ê
# ============================================
if __name__ == "__main__":
    import sys
    
    print("\n1. –ó–∞–ø—É—Å—Ç–∏—Ç—å –Ω–∞–∫—Ä—É—Ç–∫—É")
    print("2. –ü—Ä–æ—Ç–µ—Å—Ç–∏—Ä–æ–≤–∞—Ç—å –ø—Ä–æ–∫—Å–∏")
    print("3. –í—ã–π—Ç–∏")
    
    choice = input("\n–í—ã–±–µ—Ä–∏ —Ä–µ–∂–∏–º (1-3): ").strip()
    
    if choice == "1":
        asyncio.run(main())
    elif choice == "2":
        asyncio.run(test_proxies_mode())
    else:
        print("–ü–æ–∫–∞!")
