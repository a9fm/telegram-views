#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import aiohttp
import asyncio
from aiohttp_socks import ProxyConnector
import re
import random
import os
from datetime import datetime
from fake_useragent import UserAgent

# ============================================
# üìã –ù–ê–°–¢–†–û–ô–ö–ò
# ============================================
WORKING_FILE = "working.txt"
DEAD_FILE = "dead.txt"
POSTS_COUNT = 3
VIEWS_PER_POST = 10
CONCURRENCY = 100
PROXY_TIMEOUT = 5

# –ü—É—Ç–∏ –∫ —Ñ–∞–π–ª–∞–º —Å –∏—Å—Ç–æ—á–Ω–∏–∫–∞–º–∏
AUTO_HTTP = "auto/http.txt"
AUTO_SOCKS4 = "auto/socks4.txt"
AUTO_SOCKS5 = "auto/socks5.txt"

# ============================================
# üìä –°–¢–ê–¢–ò–°–¢–ò–ö–ê
# ============================================
stats = {
    'tested': 0,
    'working': 0,
    'dead': 0,
    'views_sent': 0,
    'parsed': 0,
    'start_time': datetime.now()
}

# ============================================
# üîß –í–°–ü–û–ú–û–ì–ê–¢–ï–õ–¨–ù–´–ï
# ============================================
def log(message):
    print(f"[{datetime.now().strftime('%H:%M:%S')}] {message}")

def update_progress():
    elapsed = (datetime.now() - stats['start_time']).total_seconds()
    speed = stats['tested'] / elapsed if elapsed > 0 else 0
    print(f"\rüìä –ü—Ä–æ–≥—Ä–µ—Å—Å: ‚úÖ {stats['working']} | üíÄ {stats['dead']} | üëÅÔ∏è {stats['views_sent']} | üì• {stats['parsed']} | ‚ö° {speed:.1f}/—Å | –í—Ä–µ–º—è: {elapsed:.0f}—Å", end="", flush=True)

def load_working_proxies():
    if os.path.exists(WORKING_FILE):
        with open(WORKING_FILE, "r") as f:
            return [line.strip() for line in f if line.strip()]
    return []

def save_working_proxy(proxy):
    with open(WORKING_FILE, "a") as f:
        f.write(proxy + "\n")
    stats['working'] += 1

def save_dead_proxy(proxy):
    with open(DEAD_FILE, "a") as f:
        f.write(proxy + "\n")
    stats['dead'] += 1

# ============================================
# üìñ –ß–¢–ï–ù–ò–ï –ò–°–¢–û–ß–ù–ò–ö–û–í –ò–ó –§–ê–ô–õ–û–í
# ============================================
def load_source_urls():
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç —Å—Å—ã–ª–∫–∏ –Ω–∞ –ø—Ä–æ–∫—Å–∏ –∏–∑ —Ñ–∞–π–ª–æ–≤ auto/"""
    sources = {
        'http': [],
        'socks4': [],
        'socks5': []
    }
    
    # –ß–∏—Ç–∞–µ–º http.txt
    if os.path.exists(AUTO_HTTP):
        with open(AUTO_HTTP, 'r') as f:
            sources['http'] = [line.strip() for line in f if line.strip() and not line.startswith('#')]
        log(f"üìÅ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(sources['http'])} HTTP –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤")
    
    # –ß–∏—Ç–∞–µ–º socks4.txt
    if os.path.exists(AUTO_SOCKS4):
        with open(AUTO_SOCKS4, 'r') as f:
            sources['socks4'] = [line.strip() for line in f if line.strip() and not line.startswith('#')]
        log(f"üìÅ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(sources['socks4'])} SOCKS4 –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤")
    
    # –ß–∏—Ç–∞–µ–º socks5.txt
    if os.path.exists(AUTO_SOCKS5):
        with open(AUTO_SOCKS5, 'r') as f:
            sources['socks5'] = [line.strip() for line in f if line.strip() and not line.startswith('#')]
        log(f"üìÅ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(sources['socks5'])} SOCKS5 –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤")
    
    return sources

# ============================================
# üåê –ü–ê–†–°–ò–ù–ì –ü–†–û–ö–°–ò –° –ò–°–¢–û–ß–ù–ò–ö–û–í
# ============================================
async def parse_proxies_from_url(source_url: str, proxy_type: str):
    """–ü–∞—Ä—Å–∏—Ç –ø—Ä–æ–∫—Å–∏ —Å URL –∏—Å—Ç–æ—á–Ω–∏–∫–∞"""
    try:
        async with aiohttp.ClientSession() as session:
            async with session.get(source_url, timeout=15) as resp:
                if resp.status == 200:
                    text = await resp.text()
                    # –ò—â–µ–º IP:port
                    pattern = r'\b(?:\d{1,3}\.){3}\d{1,3}:\d{2,5}\b'
                    proxies = re.findall(pattern, text)
                    
                    # –î–æ–±–∞–≤–ª—è–µ–º —Ç–∏–ø
                    result = [f"{proxy_type}://{p}" for p in proxies]
                    log(f"üì• {source_url.split('/')[-1]}: {len(result)} {proxy_type} –ø—Ä–æ–∫—Å–∏")
                    return result
    except Exception as e:
        log(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ {source_url}: {e}")
    return []

async def parse_all_proxies():
    """–ü–∞—Ä—Å–∏—Ç –ø—Ä–æ–∫—Å–∏ —Å–æ –≤—Å–µ—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ –∏–∑ auto/ —Ñ–∞–π–ª–æ–≤"""
    log("üîç –ù–∞—á–∏–Ω–∞—é –ø–∞—Ä—Å–∏–Ω–≥ –ø—Ä–æ–∫—Å–∏ –∏–∑ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤...")
    
    sources = load_source_urls()
    all_proxies = []
    tasks = []
    
    for proxy_type, urls in sources.items():
        for url in urls:
            tasks.append(parse_proxies_from_url(url, proxy_type))
    
    if not tasks:
        log("‚ùå –ù–µ—Ç –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞!")
        return []
    
    results = await asyncio.gather(*tasks)
    
    for proxies in results:
        all_proxies.extend(proxies)
    
    # –£–±–∏—Ä–∞–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã
    unique = list(set(all_proxies))
    stats['parsed'] = len(unique)
    
    log(f"üìä –í—Å–µ–≥–æ —Å–ø–∞—Ä—Å–µ–Ω–æ: {len(all_proxies)} –ø—Ä–æ–∫—Å–∏, —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö: {len(unique)}")
    return unique

# ============================================
# üîç –ü–†–û–í–ï–†–ö–ê –ü–†–û–ö–°–ò
# ============================================
async def check_proxy(proxy_url: str, test_url: str):
    """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç —Ä–∞–±–æ—Ç–∞–µ—Ç –ª–∏ –ø—Ä–æ–∫—Å–∏"""
    try:
        connector = ProxyConnector.from_url(proxy_url, rdns=True)
        async with aiohttp.ClientSession(connector=connector) as session:
            headers = {"User-Agent": UserAgent().random}
            
            async with session.get(
                test_url,
                headers=headers,
                timeout=aiohttp.ClientTimeout(total=PROXY_TIMEOUT)
            ) as response:
                
                if response.status == 200:
                    html = await response.text()
                    if 'data-view="' in html:
                        return True
        return False
    except:
        return False

async def test_proxies_batch(proxies, test_url):
    """–¢–µ—Å—Ç–∏—Ä—É–µ—Ç –ø–∞—á–∫—É –ø—Ä–æ–∫—Å–∏"""
    log(f"üß™ –¢–µ—Å—Ç–∏—Ä—É—é {len(proxies)} –ø—Ä–æ–∫—Å–∏...")
    semaphore = asyncio.Semaphore(500)  
    
    async def test_one(proxy):
        async with semaphore:
            stats['tested'] += 1
            if await check_proxy(proxy, test_url):
                save_working_proxy(proxy)
                update_progress()
                return True
            else:
                save_dead_proxy(proxy)
                update_progress()
                return False
    
    tasks = [test_one(p) for p in proxies]
    results = await asyncio.gather(*tasks)
    
    working = [p for p, r in zip(proxies, results) if r]
    log(f"\n‚úÖ –ù–∞–π–¥–µ–Ω–æ {len(working)} —Ä–∞–±–æ—á–∏—Ö –ø—Ä–æ–∫—Å–∏")
    return working

# ============================================
# üéØ –û–¢–ü–†–ê–í–ö–ê –ü–†–û–°–ú–û–¢–†–ê
# ============================================
async def send_view(channel: str, post_id: int, proxy_url: str = None):
    """–û—Ç–ø—Ä–∞–≤–ª—è–µ—Ç –ø—Ä–æ—Å–º–æ—Ç—Ä"""
    try:
        connector = None
        if proxy_url:
            connector = ProxyConnector.from_url(proxy_url, rdns=True)
        
        async with aiohttp.ClientSession(connector=connector) as session:
            ua = UserAgent().random
            
            # –ü–æ–ª—É—á–∞–µ–º —Ç–æ–∫–µ–Ω
            headers = {
                "User-Agent": ua,
                "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",
                "Accept-Language": "ru-RU,ru;q=0.9,en;q=0.8",
            }
            
            embed_url = f"https://t.me/{channel}/{post_id}?embed=1&mode=tme"
            
            async with session.get(embed_url, headers=headers, timeout=PROXY_TIMEOUT) as resp:
                if resp.status != 200:
                    return False
                
                html = await resp.text()
                token_match = re.search(r'data-view="([^"]+)"', html)
                if not token_match:
                    return False
                
                token = token_match.group(1)
                
                # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º –ø—Ä–æ—Å–º–æ—Ç—Ä
                view_headers = {
                    "User-Agent": ua,
                    "Referer": embed_url,
                    "X-Requested-With": "XMLHttpRequest",
                }
                
                async with session.post(
                    f"https://t.me/v/?views={token}",
                    headers=view_headers,
                    timeout=PROXY_TIMEOUT
                ) as view_resp:
                    
                    if view_resp.status == 200:
                        text = await view_resp.text()
                        if text == "true":
                            stats['views_sent'] += 1
                            update_progress()
                            return True
        return False
    except:
        return False

# ============================================
# üåê –ü–ê–†–°–ò–ù–ì –ü–û–°–¢–û–í
# ============================================
async def get_last_posts(channel: str):
    """–ü–æ–ª—É—á–∞–µ—Ç –ø–æ—Å–ª–µ–¥–Ω–∏–µ –ø–æ—Å—Ç—ã"""
    url = f"https://t.me/s/{channel}"
    
    async with aiohttp.ClientSession() as session:
        try:
            headers = {"User-Agent": UserAgent().random}
            async with session.get(url, headers=headers, timeout=10) as resp:
                html = await resp.text()
                
                # –ò—â–µ–º ID –ø–æ—Å—Ç–æ–≤
                pattern = rf'data-post="{channel}/(\d+)"'
                post_ids = re.findall(pattern, html)
                
                if not post_ids:
                    pattern = rf'href="/{channel}/(\d+)"'
                    post_ids = re.findall(pattern, html)
                
                if post_ids:
                    unique = sorted(set(int(id) for id in post_ids))
                    last = unique[-POSTS_COUNT:]
                    log(f"üì° –ù–∞–π–¥–µ–Ω–æ –ø–æ—Å—Ç–æ–≤: {last}")
                    return last
        except Exception as e:
            log(f"‚ùå –û—à–∏–±–∫–∞: {e}")
    return []

# ============================================
# üöÄ –†–ï–ñ–ò–ú–´ –†–ê–ë–û–¢–´
# ============================================
async def auto_mode(channel: str):
    """–†–µ–∂–∏–º AUTO - –ø–∞—Ä—Å–∏—Ç –ø—Ä–æ–∫—Å–∏ –∏–∑ auto/ —Ñ–∞–π–ª–æ–≤, —Ç–µ—Å—Ç–∏—Ä—É–µ—Ç, –∏—Å–ø–æ–ª—å–∑—É–µ—Ç"""
    log("üöÄ –ó–∞–ø—É—â–µ–Ω AUTO —Ä–µ–∂–∏–º (—Å –ø–∞—Ä—Å–∏–Ω–≥–æ–º –∏–∑ auto/ —Ñ–∞–π–ª–æ–≤)")
    
    # 1. –ü–æ–ª—É—á–∞–µ–º –ø–æ—Å—Ç—ã
    post_ids = await get_last_posts(channel)
    if not post_ids:
        log("‚ùå –ù–µ—Ç –ø–æ—Å—Ç–æ–≤")
        return
    
    # 2. –ü–∞—Ä—Å–∏–º —Å–≤–µ–∂–∏–µ –ø—Ä–æ–∫—Å–∏ –∏–∑ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤
    fresh_proxies = await parse_all_proxies()
    if not fresh_proxies:
        log("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å —Å–ø–∞—Ä—Å–∏—Ç—å –ø—Ä–æ–∫—Å–∏")
        return
    
    # 3. –¢–µ—Å—Ç–∏—Ä—É–µ–º –Ω–∞ –ø–µ—Ä–≤–æ–º –ø–æ—Å—Ç–µ
    test_url = f"https://t.me/{channel}/{post_ids[0]}?embed=1&mode=tme"
    working = await test_proxies_batch(fresh_proxies, test_url)
    
    if not working:
        log("‚ùå –ù–µ—Ç —Ä–∞–±–æ—á–∏—Ö –ø—Ä–æ–∫—Å–∏")
        return
    
    # 4. –ó–∞–ø—É—Å–∫–∞–µ–º –Ω–∞–∫—Ä—É—Ç–∫—É
    log(f"üéØ –ó–∞–ø—É—Å–∫ –Ω–∞–∫—Ä—É—Ç–∫–∏ –Ω–∞ {post_ids}")
    all_tasks = []
    for post_id in post_ids:
        for _ in range(VIEWS_PER_POST):
            proxy = random.choice(working)
            all_tasks.append(send_view(channel, post_id, proxy))
    
    semaphore = asyncio.Semaphore(CONCURRENCY)
    
    async def run_with_limit(task):
        async with semaphore:
            return await task
    
    tasks = [run_with_limit(t) for t in all_tasks]
    results = await asyncio.gather(*tasks, return_exceptions=True)
    
    success = sum(1 for r in results if r is True)
    log(f"\n‚úÖ –ù–∞–∫—Ä—É—Ç–∫–∞: {success}/{len(all_tasks)} —É—Å–ø–µ—à–Ω–æ")

async def list_mode(channel: str):
    """–†–µ–∂–∏–º LIST - –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –≥–æ—Ç–æ–≤—ã–µ –ø—Ä–æ–∫—Å–∏ –∏–∑ working.txt"""
    log("üöÄ –ó–∞–ø—É—â–µ–Ω LIST —Ä–µ–∂–∏–º")
    
    post_ids = await get_last_posts(channel)
    if not post_ids:
        log("‚ùå –ù–µ—Ç –ø–æ—Å—Ç–æ–≤")
        return
    
    proxies = load_working_proxies()
    if not proxies:
        log("‚ùå –ù–µ—Ç –ø—Ä–æ–∫—Å–∏ –≤ working.txt")
        return
    
    log(f"‚úÖ –ò—Å–ø–æ–ª—å–∑—É—é {len(proxies)} –≥–æ—Ç–æ–≤—ã—Ö –ø—Ä–æ–∫—Å–∏")
    log(f"üéØ –ü–æ—Å—Ç—ã: {post_ids}")
    
    all_tasks = []
    for post_id in post_ids:
        for _ in range(VIEWS_PER_POST):
            proxy = random.choice(proxies)
            all_tasks.append(send_view(channel, post_id, proxy))
    
    semaphore = asyncio.Semaphore(CONCURRENCY)
    
    async def run_with_limit(task):
        async with semaphore:
            return await task
    
    tasks = [run_with_limit(t) for t in all_tasks]
    results = await asyncio.gather(*tasks, return_exceptions=True)
    
    success = sum(1 for r in results if r is True)
    log(f"\n‚úÖ –ù–∞–∫—Ä—É—Ç–∫–∞: {success}/{len(all_tasks)} —É—Å–ø–µ—à–Ω–æ")

# ============================================
# üìå –¢–û–ß–ö–ê –í–•–û–î–ê
# ============================================
if __name__ == "__main__":
    import sys
    import argparse
    
    parser = argparse.ArgumentParser()
    parser.add_argument("-c", "--channel", help="–ö–∞–Ω–∞–ª –±–µ–∑ @")
    parser.add_argument("-m", "--mode", help="auto –∏–ª–∏ list")
    args = parser.parse_args()
    
    if not args.channel:
        args.channel = input("üì¢ –ö–∞–Ω–∞–ª (–±–µ–∑ @): ").strip()
    
    if not args.mode:
        print("\n1. Auto —Ä–µ–∂–∏–º (–ø–∞—Ä—Å–∏–Ω–≥ –∏–∑ auto/ + —Ç–µ—Å—Ç + –Ω–∞–∫—Ä—É—Ç–∫–∞)")
        print("2. List —Ä–µ–∂–∏–º (—Ç–æ–ª—å–∫–æ –Ω–∞–∫—Ä—É—Ç–∫–∞ –∏–∑ working.txt)")
        choice = input("\n–í—ã–±–µ—Ä–∏ (1/2): ").strip()
        args.mode = "auto" if choice == "1" else "list"
    
    print("=" * 50)
    print(f"ü§ñ Telegram Views Bot - {args.mode.upper()} —Ä–µ–∂–∏–º")
    print("=" * 50)
    
    stats['start_time'] = datetime.now()
    
    if args.mode == "auto":
        asyncio.run(auto_mode(args.channel))
    else:
        asyncio.run(list_mode(args.channel))
    
    elapsed = (datetime.now() - stats['start_time']).total_seconds()
    print("\n" + "=" * 50)
    print("üèÅ –ì–û–¢–û–í–û")
    print(f"‚úÖ –†–∞–±–æ—á–∏—Ö –ø—Ä–æ–∫—Å–∏: {stats['working']}")
    print(f"üíÄ –ú–µ—Ä—Ç–≤—ã—Ö –ø—Ä–æ–∫—Å–∏: {stats['dead']}")
    print(f"üëÅÔ∏è –ü—Ä–æ—Å–º–æ—Ç—Ä–æ–≤: {stats['views_sent']}")
    print(f"üì• –°–ø–∞—Ä—Å–µ–Ω–æ: {stats['parsed']}")
    print(f"‚è±Ô∏è –í—Ä–µ–º—è: {elapsed:.1f}—Å")
    print("=" * 50)
