#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import aiohttp
import asyncio
from aiohttp_socks import ProxyConnector
import re
import random
import os
from datetime import datetime
from fake_useragent import UserAgent

# ============================================
# üìã –ù–ê–°–¢–†–û–ô–ö–ò
# ============================================
WORKING_FILE = "working.txt"
DEAD_FILE = "dead.txt"
POSTS_COUNT = 3
CONCURRENCY = 300
PROXY_TIMEOUT = 10
MAX_USES_PER_PROXY = 5

# –ü—É—Ç–∏ –∫ —Ñ–∞–π–ª–∞–º —Å –∏—Å—Ç–æ—á–Ω–∏–∫–∞–º–∏
AUTO_HTTP = "auto/http.txt"
AUTO_SOCKS4 = "auto/socks4.txt"
AUTO_SOCKS5 = "auto/socks5.txt"

# ============================================
# üìä –°–¢–ê–¢–ò–°–¢–ò–ö–ê
# ============================================
stats = {
    'tested': 0,
    'working': 0,
    'dead': 0,
    'views_sent': 0,
    'parsed': 0,
    'start_time': datetime.now()
}

# ============================================
# üîß –í–°–ü–û–ú–û–ì–ê–¢–ï–õ–¨–ù–´–ï
# ============================================
def log(message):
    print(f"[{datetime.now().strftime('%H:%M:%S')}] {message}")

def update_progress():
    elapsed = (datetime.now() - stats['start_time']).total_seconds()
    speed = stats['tested'] / elapsed if elapsed > 0 else 0
    print(f"\rüìä –ü—Ä–æ–≥—Ä–µ—Å—Å: ‚úÖ {stats['working']} | üíÄ {stats['dead']} | üëÅÔ∏è {stats['views_sent']} | üì• {stats['parsed']} | ‚ö° {speed:.1f}/—Å | –í—Ä–µ–º—è: {elapsed:.0f}—Å", end="", flush=True)

def load_working_proxies():
    if os.path.exists(WORKING_FILE):
        with open(WORKING_FILE, "r") as f:
            return [line.strip() for line in f if line.strip()]
    return []

def save_working_proxy(proxy):
    with open(WORKING_FILE, "a") as f:
        f.write(proxy + "\n")
    stats['working'] += 1

def save_dead_proxy(proxy):
    with open(DEAD_FILE, "a") as f:
        f.write(proxy + "\n")
    stats['dead'] += 1

def load_dead_proxies():
    if os.path.exists(DEAD_FILE):
        with open(DEAD_FILE, "r") as f:
            return set(line.strip() for line in f if line.strip())
    return set()

# ============================================
# üìñ –ß–¢–ï–ù–ò–ï –ò–°–¢–û–ß–ù–ò–ö–û–í –ò–ó –§–ê–ô–õ–û–í
# ============================================
def load_source_urls():
    sources = {'http': [], 'socks4': [], 'socks5': []}
    
    if os.path.exists(AUTO_HTTP):
        with open(AUTO_HTTP, 'r') as f:
            sources['http'] = [line.strip() for line in f if line.strip() and not line.startswith('#')]
        log(f"üìÅ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(sources['http'])} HTTP –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤")
    
    if os.path.exists(AUTO_SOCKS4):
        with open(AUTO_SOCKS4, 'r') as f:
            sources['socks4'] = [line.strip() for line in f if line.strip() and not line.startswith('#')]
        log(f"üìÅ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(sources['socks4'])} SOCKS4 –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤")
    
    if os.path.exists(AUTO_SOCKS5):
        with open(AUTO_SOCKS5, 'r') as f:
            sources['socks5'] = [line.strip() for line in f if line.strip() and not line.startswith('#')]
        log(f"üìÅ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(sources['socks5'])} SOCKS5 –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤")
    
    return sources

# ============================================
# üåê –ü–ê–†–°–ò–ù–ì –ü–†–û–ö–°–ò –° –ò–°–¢–û–ß–ù–ò–ö–û–í
# ============================================
async def parse_proxies_from_url(source_url: str, proxy_type: str):
    try:
        async with aiohttp.ClientSession() as session:
            async with session.get(source_url, timeout=30) as resp:
                if resp.status == 200:
                    text = await resp.text()
                    pattern = r'\b(?:\d{1,3}\.){3}\d{1,3}:\d{2,5}\b'
                    proxies = re.findall(pattern, text)
                    result = [f"{proxy_type}://{p}" for p in proxies]
                    source_name = source_url.split('/')[-1] or source_url.split('/')[-2]
                    log(f"üì• {source_name}: {len(result)} {proxy_type} –ø—Ä–æ–∫—Å–∏")
                    return result
    except Exception as e:
        log(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ {source_url}: {str(e)[:50]}")
    return []

async def parse_all_proxies():
    log("üîç –ù–∞—á–∏–Ω–∞—é –ø–∞—Ä—Å–∏–Ω–≥ –ø—Ä–æ–∫—Å–∏ –∏–∑ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤...")
    
    sources = load_source_urls()
    all_proxies = []
    tasks = []
    
    for proxy_type, urls in sources.items():
        for url in urls:
            tasks.append(parse_proxies_from_url(url, proxy_type))
    
    results = await asyncio.gather(*tasks)
    
    for proxies in results:
        all_proxies.extend(proxies)
    
    unique = list(set(all_proxies))
    stats['parsed'] = len(unique)
    
    dead_set = load_dead_proxies()
    alive = [p for p in unique if p not in dead_set]
    
    log(f"üìä –í—Å–µ–≥–æ: {len(all_proxies)} ‚Üí —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö: {len(unique)} ‚Üí –±–µ–∑ –º–µ—Ä—Ç–≤—ã—Ö: {len(alive)}")
    return alive

# ============================================
# üîç –ü–†–û–í–ï–†–ö–ê –ü–†–û–ö–°–ò
# ============================================
async def check_proxy(proxy_url: str, test_url: str):
    try:
        connector = ProxyConnector.from_url(proxy_url, rdns=True)
        async with aiohttp.ClientSession(connector=connector) as session:
            headers = {
                "User-Agent": UserAgent().random,
                "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",
                "Accept-Language": "ru-RU,ru;q=0.9,en;q=0.8",
            }
            async with session.get(
                test_url, 
                headers=headers, 
                timeout=aiohttp.ClientTimeout(total=PROXY_TIMEOUT)
            ) as response:
                if response.status == 200:
                    html = await response.text()
                    if 'tgme_' in html or 'data-view=' in html or 'telegram' in html.lower():
                        return True
    except:
        pass
    return False

async def test_proxies_batch(proxies, test_url):
    log(f"üß™ –¢–µ—Å—Ç–∏—Ä—É—é {len(proxies)} –ø—Ä–æ–∫—Å–∏...")
    
    dead_set = load_dead_proxies()
    proxies = [p for p in proxies if p not in dead_set]
    log(f"üìâ –ü–æ—Å–ª–µ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏ –º–µ—Ä—Ç–≤—ã—Ö: {len(proxies)}")
    
    semaphore = asyncio.Semaphore(300)
    working = []
    
    async def test_one(proxy):
        async with semaphore:
            stats['tested'] += 1
            if await check_proxy(proxy, test_url):
                save_working_proxy(proxy)
                working.append(proxy)
                update_progress()
                return True
            else:
                save_dead_proxy(proxy)
                update_progress()
                return False
    
    chunk_size = 2000
    for i in range(0, len(proxies), chunk_size):
        chunk = proxies[i:i+chunk_size]
        tasks = [test_one(p) for p in chunk]
        await asyncio.gather(*tasks)
        log(f"üìä –ß–∞–Ω–∫ {i//chunk_size + 1}: –Ω–∞–π–¥–µ–Ω–æ {len(working)} —Ä–∞–±–æ—á–∏—Ö")
        await asyncio.sleep(0.5)
    
    log(f"\n‚úÖ –ù–∞–π–¥–µ–Ω–æ {len(working)} —Ä–∞–±–æ—á–∏—Ö –ø—Ä–æ–∫—Å–∏")
    return working

# ============================================
# üéØ –û–¢–ü–†–ê–í–ö–ê –ü–†–û–°–ú–û–¢–†–ê
# ============================================
async def send_view(channel: str, post_id: int, proxy_url: str = None):
    try:
        connector = None
        if proxy_url:
            connector = ProxyConnector.from_url(proxy_url, rdns=True)
        
        async with aiohttp.ClientSession(connector=connector) as session:
            ua = UserAgent().random
            
            headers = {
                "User-Agent": ua,
                "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",
                "Accept-Language": "ru-RU,ru;q=0.9,en;q=0.8",
                "Accept-Encoding": "gzip, deflate, br",
                "Connection": "keep-alive",
                "Upgrade-Insecure-Requests": "1",
            }
            
            embed_url = f"https://t.me/{channel}/{post_id}?embed=1&mode=tme"
            
            async with session.get(embed_url, headers=headers, timeout=PROXY_TIMEOUT) as resp:
                if resp.status != 200:
                    return False
                
                html = await resp.text()
                
                token_match = re.search(r'data-view="([^"]+)"', html)
                if not token_match:
                    token_match = re.search(r'data-view=\'([^\']+)\'', html)
                if not token_match:
                    token_match = re.search(r'data-view=([^>\s]+)', html)
                
                if not token_match:
                    return False
                
                token = token_match.group(1).strip('"').strip("'")
                
                view_headers = {
                    "User-Agent": ua,
                    "Referer": embed_url,
                    "X-Requested-With": "XMLHttpRequest",
                    "Accept": "*/*",
                    "Accept-Language": "ru-RU,ru;q=0.9,en;q=0.8",
                }
                
                async with session.post(
                    f"https://t.me/v/?views={token}",
                    headers=view_headers,
                    timeout=PROXY_TIMEOUT
                ) as view_resp:
                    
                    if view_resp.status == 200:
                        text = await view_resp.text()
                        if text == "true" or "true" in text:
                            stats['views_sent'] += 1
                            update_progress()
                            if proxy_url:
                                with open("gold_proxies.txt", "a") as f:
                                    f.write(proxy_url + "\n")
                            return True
        return False
    except:
        return False

# ============================================
# üåê –ü–ê–†–°–ò–ù–ì –ü–û–°–¢–û–í
# ============================================
async def get_last_posts(channel: str):
    url = f"https://t.me/s/{channel}"
    
    async with aiohttp.ClientSession() as session:
        try:
            headers = {"User-Agent": UserAgent().random}
            async with session.get(url, headers=headers, timeout=15) as resp:
                if resp.status != 200:
                    log(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –∫–∞–Ω–∞–ª–∞: {resp.status}")
                    return []
                
                html = await resp.text()
                
                patterns = [
                    rf'data-post="{channel}/(\d+)"',
                    rf'href="/{channel}/(\d+)"',
                    rf'data-post="//t.me/{channel}/(\d+)"',
                    rf'tgme_widget_message[^>]*data-post="{channel}/(\d+)"',
                ]
                
                post_ids = []
                for pattern in patterns:
                    found = re.findall(pattern, html)
                    post_ids.extend(found)
                
                if not post_ids:
                    log("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –Ω–∞–π—Ç–∏ –ø–æ—Å—Ç—ã")
                    return []
                
                unique = sorted(set(int(id) for id in post_ids))
                last = unique[-POSTS_COUNT:] if len(unique) >= POSTS_COUNT else unique
                
                log(f"üì° –ù–∞–π–¥–µ–Ω–æ –ø–æ—Å—Ç–æ–≤: {last}")
                return last
                
        except Exception as e:
            log(f"‚ùå –û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞: {e}")
            return []

# ============================================
# üöÄ –†–ï–ñ–ò–ú LIST - –ö–†–£–¢–ò–¢ –ù–ê –í–°–ï 3 –ü–û–°–¢–ê –¶–ò–ö–õ–û–ú!
# ============================================
async def list_mode(channel: str):
    log("üöÄ –ó–∞–ø—É—â–µ–Ω LIST —Ä–µ–∂–∏–º - –∫—Ä—É—á—É –Ω–∞ –í–°–ï 3 –ø–æ—Å—Ç–∞ —Ü–∏–∫–ª–æ–º!")
    
    # –ü–æ–ª—É—á–∞–µ–º –ø–æ—Å—Ç—ã - –í–°–ï 3 –®–¢–£–ö–ò
    post_ids = await get_last_posts(channel)
    if not post_ids:
        log("‚ùå –ù–µ—Ç –ø–æ—Å—Ç–æ–≤")
        return
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º –ø—Ä–æ–∫—Å–∏
    proxies = load_working_proxies()
    if not proxies:
        log("‚ùå –ù–µ—Ç –ø—Ä–æ–∫—Å–∏ –≤ working.txt")
        return
    
    log(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(proxies)} –ø—Ä–æ–∫—Å–∏")
    log(f"üéØ –ü–û–°–¢–´ –î–õ–Ø –ù–ê–ö–†–£–¢–ö–ò: {post_ids}")  # –Ø–í–ù–û –ü–û–ö–ê–ó–´–í–ê–Æ –í–°–ï –ü–û–°–¢–´
    log(f"üîÑ –ö–∞–∂–¥—ã–π –ø—Ä–æ–∫—Å–∏ –º–æ–∂–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å {MAX_USES_PER_PROXY} —Ä–∞–∑")
    
    # –°–ª–æ–≤–∞—Ä—å –¥–ª—è –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–Ω—ã—Ö –ø—Ä–æ–∫—Å–∏
    proxy_usage = {proxy: 0 for proxy in proxies}
    total_attempts = 0
    successful_views = 0
    
    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –ø–æ—Å—Ç–∞–º
    post_stats = {post_id: {'attempts': 0, 'success': 0} for post_id in post_ids}
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º –∑–æ–ª–æ—Ç—ã–µ –ø—Ä–æ–∫—Å–∏ –µ—Å–ª–∏ –µ—Å—Ç—å
    gold_proxies = []
    if os.path.exists("gold_proxies.txt"):
        with open("gold_proxies.txt", "r") as f:
            gold_proxies = [line.strip() for line in f if line.strip()]
        log(f"‚ú® –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(gold_proxies)} –∑–æ–ª–æ—Ç—ã—Ö –ø—Ä–æ–∫—Å–∏")
    
    try:
        cycle = 1
        while True:
            log(f"\nüîÑ –¶–ò–ö–õ {cycle} - –ù–∞–∫—Ä—É—Ç–∫–∞ –Ω–∞ –ø–æ—Å—Ç—ã: {post_ids}")
            
            # –ü—Ä–æ—Ö–æ–¥–∏–º –ø–æ –ö–ê–ñ–î–û–ú–£ –ü–û–°–¢–£ –ø–æ –æ—á–µ—Ä–µ–¥–∏
            for post_id in post_ids:
                # –í—ã–±–∏—Ä–∞–µ–º –ø—Ä–æ–∫—Å–∏
                if gold_proxies and random.random() > 0.5:
                    available_proxies = gold_proxies
                else:
                    available_proxies = [p for p in proxies if proxy_usage[p] < MAX_USES_PER_PROXY]
                
                if not available_proxies and not gold_proxies:
                    log(f"‚ùå –í—Å–µ –ø—Ä–æ–∫—Å–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω—ã {MAX_USES_PER_PROXY} —Ä–∞–∑")
                    break
                
                # –ë–µ—Ä–µ–º –ø—Ä–æ–∫—Å–∏
                proxy = random.choice(available_proxies) if available_proxies else random.choice(gold_proxies) if gold_proxies else None
                
                if proxy in proxy_usage:
                    proxy_usage[proxy] += 1
                
                total_attempts += 1
                post_stats[post_id]['attempts'] += 1
                
                # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º –ø—Ä–æ—Å–º–æ—Ç—Ä
                result = await send_view(channel, post_id, proxy)
                
                if result:
                    successful_views += 1
                    post_stats[post_id]['success'] += 1
            
            # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ—Å–ª–µ –∫–∞–∂–¥–æ–≥–æ —Ü–∏–∫–ª–∞
            elapsed = (datetime.now() - stats['start_time']).total_seconds()
            success_rate = (successful_views / total_attempts * 100) if total_attempts > 0 else 0
            
            log(f"üìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ—Å–ª–µ —Ü–∏–∫–ª–∞ {cycle}:")
            log(f"   üëÅÔ∏è –í—Å–µ–≥–æ –ø—Ä–æ—Å–º–æ—Ç—Ä–æ–≤: {successful_views}")
            log(f"   üéØ –ü–æ—Å—Ç—ã:")
            for post_id in post_ids:
                post_success = post_stats[post_id]['success']
                post_attempts = post_stats[post_id]['attempts']
                post_rate = (post_success / post_attempts * 100) if post_attempts > 0 else 0
                log(f"      - –ø–æ—Å—Ç {post_id}: {post_success}/{post_attempts} ({post_rate:.1f}%)")
            log(f"   üìà –û–±—â–∏–π –ø—Ä–æ—Ü–µ–Ω—Ç: {success_rate:.1f}%")
            log(f"   ‚è±Ô∏è –í—Ä–µ–º—è: {elapsed:.0f}—Å")
            
            cycle += 1
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –æ—Å—Ç–∞–ª–∏—Å—å –ª–∏ –ø—Ä–æ–∫—Å–∏
            available = [p for p in proxies if proxy_usage[p] < MAX_USES_PER_PROXY]
            if not available and not gold_proxies:
                log("‚ùå –í—Å–µ –ø—Ä–æ–∫—Å–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω—ã. –ó–∞–≤–µ—Ä—à–∞—é —Ä–∞–±–æ—Ç—É.")
                break
            
            # –ù–µ–±–æ–ª—å—à–∞—è –ø–∞—É–∑–∞ –º–µ–∂–¥—É —Ü–∏–∫–ª–∞–º–∏
            await asyncio.sleep(1)
            
    except KeyboardInterrupt:
        log("\nüõë –û—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º")
    
    # –ò–¢–û–ì–û–í–ê–Ø –°–¢–ê–¢–ò–°–¢–ò–ö–ê –ü–û –í–°–ï–ú –ü–û–°–¢–ê–ú
    elapsed = (datetime.now() - stats['start_time']).total_seconds()
    success_rate = (successful_views / total_attempts * 100) if total_attempts > 0 else 0
    
    log(f"\n{'='*60}")
    log(f"üèÅ –ò–¢–û–ì–û–í–ê–Ø –°–¢–ê–¢–ò–°–¢–ò–ö–ê")
    log(f"{'='*60}")
    log(f"üìä –û–ë–©–ï–ï:")
    log(f"   ‚úÖ –í—Å–µ–≥–æ –ø—Ä–æ—Å–º–æ—Ç—Ä–æ–≤: {successful_views}")
    log(f"   üîÑ –í—Å–µ–≥–æ –ø–æ–ø—ã—Ç–æ–∫: {total_attempts}")
    log(f"   üìà –ü—Ä–æ—Ü–µ–Ω—Ç —É—Å–ø–µ—Ö–∞: {success_rate:.1f}%")
    log(f"   ‚è±Ô∏è –í—Ä–µ–º—è —Ä–∞–±–æ—Ç—ã: {elapsed:.1f}—Å")
    log(f"\nüéØ –ü–û–°–¢–ê–ú:")
    for post_id in post_ids:
        post_success = post_stats[post_id]['success']
        post_attempts = post_stats[post_id]['attempts']
        post_rate = (post_success / post_attempts * 100) if post_attempts > 0 else 0
        log(f"   - –ø–æ—Å—Ç {post_id}: {post_success} –ø—Ä–æ—Å–º–æ—Ç—Ä–æ–≤ –∏–∑ {post_attempts} –ø–æ–ø—ã—Ç–æ–∫ ({post_rate:.1f}%)")
    log(f"\n‚ú® –ó–æ–ª–æ—Ç—ã—Ö –ø—Ä–æ–∫—Å–∏: {len(gold_proxies)}")
    log(f"{'='*60}")

# ============================================
# üöÄ –†–ï–ñ–ò–ú AUTO
# ============================================
async def auto_mode(channel: str):
    log("üöÄ –ó–∞–ø—É—â–µ–Ω AUTO —Ä–µ–∂–∏–º")
    
    post_ids = await get_last_posts(channel)
    if not post_ids:
        log("‚ùå –ù–µ—Ç –ø–æ—Å—Ç–æ–≤")
        return
    
    fresh_proxies = await parse_all_proxies()
    if not fresh_proxies:
        log("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å —Å–ø–∞—Ä—Å–∏—Ç—å –ø—Ä–æ–∫—Å–∏")
        return
    
    test_url = f"https://t.me/{channel}/{post_ids[0]}?embed=1&mode=tme"
    working = await test_proxies_batch(fresh_proxies, test_url)
    
    if not working:
        log("‚ùå –ù–µ—Ç —Ä–∞–±–æ—á–∏—Ö –ø—Ä–æ–∫—Å–∏")
        return
    
    log(f"‚úÖ –ù–∞–π–¥–µ–Ω–æ {len(working)} —Ä–∞–±–æ—á–∏—Ö –ø—Ä–æ–∫—Å–∏, –∑–∞–ø—É—Å–∫–∞—é –Ω–∞–∫—Ä—É—Ç–∫—É...")
    
    with open(WORKING_FILE, "w") as f:
        for proxy in working:
            f.write(proxy + "\n")
    
    await list_mode(channel)

# ============================================
# üìå –¢–û–ß–ö–ê –í–•–û–î–ê
# ============================================
if __name__ == "__main__":
    import sys
    
    print("=" * 60)
    print("ü§ñ TELEGRAM VIEWS BOT - –ö–†–£–¢–ò–¢ –ù–ê –í–°–ï 3 –ü–û–°–¢–ê!")
    print("=" * 60)
    
    channel = input("üì¢ –ö–∞–Ω–∞–ª (–±–µ–∑ @): ").strip()
    if not channel:
        channel = "a9fm_price"
        print(f"‚ö†Ô∏è –ò—Å–ø–æ–ª—å–∑—É—é {channel}")
    
    print("\n1. Auto —Ä–µ–∂–∏–º (–ø–∞—Ä—Å–∏–Ω–≥ –∏–∑ auto/ + —Ç–µ—Å—Ç + –Ω–∞–∫—Ä—É—Ç–∫–∞)")
    print("2. List —Ä–µ–∂–∏–º (—Ç–æ–ª—å–∫–æ –Ω–∞–∫—Ä—É—Ç–∫–∞ –∏–∑ working.txt)")
    print("3. –¢–µ—Å—Ç –ø—Ä–æ–∫—Å–∏ (—Ç–æ–ª—å–∫–æ –ø—Ä–æ–≤–µ—Ä–∏—Ç—å –∏ —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å —Ä–∞–±–æ—á–∏–µ)")
    
    choice = input("\n–í—ã–±–µ—Ä–∏ (1/2/3): ").strip()
    
    print("=" * 60)
    stats['start_time'] = datetime.now()
    
    if choice == "1":
        print("ü§ñ AUTO –†–ï–ñ–ò–ú")
        print("=" * 60)
        asyncio.run(auto_mode(channel))
    elif choice == "2":
        print("ü§ñ LIST –†–ï–ñ–ò–ú - –ö–†–£–ß–£ –í–°–ï 3 –ü–û–°–¢–ê!")
        print("=" * 60)
        asyncio.run(list_mode(channel))
    elif choice == "3":
        print("üîç –¢–ï–°–¢ –ü–†–û–ö–°–ò")
        print("=" * 60)
        post_ids = asyncio.run(get_last_posts(channel))
        if post_ids:
            test_url = f"https://t.me/{channel}/{post_ids[0]}?embed=1&mode=tme"
            fresh_proxies = asyncio.run(parse_all_proxies())
            if fresh_proxies:
                asyncio.run(test_proxies_batch(fresh_proxies, test_url))
    else:
        print("‚ùå –ù–µ–≤–µ—Ä–Ω—ã–π –≤—ã–±–æ—Ä")
